{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Watson Assistant Metrics Notebook\n\nThis notebook performs analytics on the user log records of Watson Assistant (including Voice Interaction). The logs are extracted, transformed, and loaded into a DB2 database and CSV files. A variety of key business metrics are calculated and displayed in the notebook. Using Watson Studio to build a Dashboard are recommended for further data exploration and dashboard visualizations. \n\n<br> <div> <img src=\"https://github.com/preethm/watson-assistant-metrics-notebook/blob/master/Data%20Flow%20Diagram.png?raw=true\" width=\"750\"/> </div>          \n\n### Table of Contents\n* [1. Configuration and Log Collection](#config) - This section will extract and transform the user log data from Watson Assistant.\n* [2. Key Performance Metrics](#performance-metrics) - Key metrics including containment rate, active users, and top intents will be calculated. \n* [3. Export Logs](#export-logs) The transformed log data will be saved to a DB2 database and CSV files."}, {"metadata": {}, "cell_type": "markdown", "source": "## Housekeeping <a class=\"anchor\" id=\"housekeeping\"></a>\nThis section will import libraries and dependencies for this notebook. \n \n> **Action Required:** Update the `project_id` and `project_access_token` in order to access your data assets. Instructions can be found here: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/token.html"}, {"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='XXXXXXXX', project_access_token='XXXXXXXXX')\npc = project.project_context\n", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "!curl -O https://raw.githubusercontent.com/cognitive-catalyst/WA-Testing-Tool/master/log_analytics/getAllLogs.py\n!curl -O https://raw.githubusercontent.com/cognitive-catalyst/WA-Testing-Tool/master/log_analytics/extractConversations.py\n!curl -O https://raw.githubusercontent.com/pratyushsingh97/Bigrams/master/bigrams.py\n\n%load_ext autoreload\n%autoreload 2\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n!pip install ibm-watson\n!pip install --user --upgrade \"pandas==1.0.3\";\n!pip install -r https://raw.githubusercontent.com/pratyushsingh97/Bigrams/master/requirements.txt\n\nimport json\nimport pandas as pd\nimport getAllLogs\nimport extractConversations\nimport bigrams\nimport seaborn as sn\nimport ibm_db\nimport ibm_db_dbi", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Custom functions to re-use code throughout notebook\ndef turn_dict_to_df(df,col_names):\n    df = pd.DataFrame.from_dict(df)\n    df.reset_index(level=0, inplace=True)\n    df.columns = col_names\n    return df", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 1. Configuration and log collection <a class=\"anchor\" id=\"config\"></a>\nThis section will configure your DB2 connection, log query parameters, and will extract the logs from your Watson Assistant instance.\n\n> **Action Required:** Update each of the variables marked with 'XXXXXXXX'.  The comments in the cells guide you in the configuration."}, {"metadata": {}, "cell_type": "code", "source": "# Define the customer name. This prefix will be used for saving CSV & JSON files.\ncustName = 'XXXXXXXX'\n\n# Set True or False if you want data to write to DB2 table\nconnectDB2 = True\n\n# Set the start date for the log fetch. If you are using the DB2 connection in Section 1.1, this will be defined automatically.\nlog_fetch_start = '2020-05-15'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.1 Configure & Establish DB2 connection\nThis section will define the values for your DB2 database, establish the connection, check for the last loaded date.\n\n> **Action Required:** You will first need to provision your DB2 instance and establish the table schemas. Follow these instructions. Then, update the values below marked 'XXXXXXXX'. "}, {"metadata": {}, "cell_type": "code", "source": "# Enter the values for you database connection. This can be found in DB2's Service Credentials from the tooling. \ndsn_database = \"XXXXXXXX\"            # e.g. \"MORTGAGE\"\ndsn_uid      = \"XXXXXXXX\"            # e.g. \"dash104434\"\ndsn_pwd      = \"XXXXXXXX\"            # e.g. \"7dBZ3jWt9xN6$o0JiX!m\"\ndsn_hostname = \"XXXXXXXX\"            # e.g. \"Use the same IP as Web Console\"\ndsn_port     = \"50000\"               # e.g. \"50000\" \ndsn_protocol = \"TCPIP\"               # i.e. \"TCPIP\"\ndsn_driver   = \"IBM DB2 ODBC DRIVER\" # Don't change", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Establish database connection\nif connectDB2 == True:\n    dsn = (\"DRIVER={{IBM DB2 ODBC DRIVER}};\" \"DATABASE={0};\" \"HOSTNAME={1};\" \"PORT={2};\" \"PROTOCOL=TCPIP;\" \"UID={3};\" \"PWD={4};\").format(dsn_database, dsn_hostname, dsn_port, dsn_uid, dsn_pwd)\n    options = { ibm_db.SQL_ATTR_AUTOCOMMIT:  ibm_db.SQL_AUTOCOMMIT_ON }\n    conn = ibm_db.connect(dsn, \"\", \"\",options)\n    #Added options for auto commit\n    \n    # Retrieve the date for the previous DB2 run. If there is none defined, use 2020-04-15. This variable log_fetch_start is used for filtering WA logs.\n    select_sql = 'SELECT * FROM WATSON.WA_LAST_RUN_LOG'\n    select_stmt = ibm_db.exec_immediate(conn, select_sql)\n    prev_run = ibm_db.fetch_both(select_stmt)\n    first_run = True\n    log_fetch_start = '2020-04-15'\n    if prev_run != False:\n        first_run = False\n        l_conversation_id = prev_run.get('CONVERSATION_ID')\n        l_request_timestamp = prev_run.get('REQUEST_TIMESTAMP')\n        l_response_timestamp = prev_run.get('RESPONSE_TIMESTAMP')\n        l_prev_run = prev_run.get('LASTRUN_TIMESTAMP')\n        log_fetch_start = str(l_response_timestamp.date())\nprint('log_fetch_start:',log_fetch_start)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Retrieve logs from the Watson Assistant instance\nThis section will retrieve the user logs from the Assistant `/logs` API.\n\n> **Action Required:** Update the fields below marked 'XXXXXXXX' based on the credentials of your Assistant. \nSolutions using an Assistant layer (v2 API) should set `workspace_id=None` and provide `assistant_id`. Otherwise, define workspace and comment out assistant_id.\n\n\n"}, {"metadata": {}, "cell_type": "code", "source": "# Extract logs from your assistant. Complete this information.\niam_apikey = 'XXXXXXXX' \nurl = \"XXXXXXXX\" # Set the URL to the region, e.g. https://api.us-east.assistant.watson.cloud.ibm.com\nassistant_id = 'XXXXXXXX'\nworkspace_id = None\n\n# If not using assistant_id, comment out the 2nd line below. \nlog_filter=\"language::en,response_timestamp>=\" + log_fetch_start \\\n+\",request.context.system.assistant_id::\" + assistant_id\n\n#Change the number of logs retrieved, default settings will return 100,000 logs (200 pages of 500)\npage_size_limit=500\npage_num_limit=200\n\nversion=\"2018-09-20\" # Watson Assistant API version\n\nrawLogsJson = getAllLogs.getLogs(iam_apikey, url, workspace_id, log_filter, page_size_limit, page_num_limit, version)\nrawLogsPath= custName + \"_logs.json\"\n\n# getAllLogs.writeLogs(rawLogsJson, rawLogsPath) # Saves the logs locally\nproject.save_data(file_name = rawLogsPath,data = json.dumps(rawLogsJson),overwrite=True); # Saves the logs in Studio/COS\nprint('\\nSaved log data to {}'.format(rawLogsPath))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.3 Load logs from JSON file (Defunct)\nIf you have previously saved the JSON file, you can uncomment this section to load the logs. Otherwise, comment this section out and continue."}, {"metadata": {}, "cell_type": "code", "source": "# #If you have previously stored your logs on the file system, you can reload them here by uncommenting these lines\n# rawLogsPath= custName+\"_logs.json\"\n# rawLogsJson = extractConversations.readLogs(rawLogsPath)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.4 Format logs\nNow that the logs have been retrieved, this section will transform the data out of JSON format and into a Pandas dataframe. \n\n> **Optional:** If you wish to add any custom fields (such as a context variable), add it the first line `customFieldNames` below. Otherwise, run this cell as-is."}, {"metadata": {}, "cell_type": "code", "source": "# Optionally provide a comma-separated list of custom fields you want to extract, in addition to the default fields\ncustomFieldNames = ''\n\n# Unique conversation identifier across all records. This is default. For a multi-skill assistant you will need to provide your own key.\nprimaryLogKey = \"response.context.conversation_id\"\nconversationKey='conversation_id' # Name of the correlating key as it appears in the data frame columns (remove 'response.context.')\n\n# These custom fields are added to the list. They are used for extracting metrics in the notebook. Do not change these.\ncustomFieldNames = customFieldNames + \",response.context.vgwSIPFromURI,response.context.vgwSessionID,request.context.vgwSMSFailureReason,\\\nrequest.context.vgwSMSUserPhoneNumber,response.output.vgwAction.parameters.transferTarget,response.context.language,\\\nresponse.context.metadata.user_id,response.output.generic\"\n\nallLogsDF = extractConversations.extractConversationData(rawLogsJson, primaryLogKey, customFieldNames)\nconversationsGroup = allLogsDF.groupby(conversationKey,as_index=False)\n\n# Splits the response_timestamp into month, day, and year fields that can be used for easier data filtering/visualizations \nallLogsDF[\"full_date\"] = pd.to_datetime(allLogsDF[\"response_timestamp\"])\nallLogsDF['month'] = allLogsDF['full_date'].dt.month\nallLogsDF['day'] = allLogsDF['full_date'].dt.day\nallLogsDF['year'] = allLogsDF['full_date'].dt.year\n\nprint(\"Total log events:\",len(allLogsDF))\nallLogsDF.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Print the column names\n# allLogsDF.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.5 Extract Bigrams from Input Text\nA bigram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A Python script will be used to extract the bigrams in its lemmatization form. For example \"I am eating pie\" and \"I eat pie\" result in the same bigram \"eat_pie\". This cell will extract the bigrams from the `input.text` column and load it into a `input_bigrams` column. This column can be used to create efficient data visualizations (e.g. word clouds)."}, {"metadata": {}, "cell_type": "code", "source": "bigrams_list = bigrams.runner(allLogsDF['input.text'], stopwords=[\"like\"]) # add \"like\" to the list of stopwords\nallLogsDF['input_bigrams'] = bigrams_list", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 2. Key Performance Metrics <a class=\"anchor\" id=\"performance-metrics\"></a>\nThe notebook will calculate various performance metrics including `coverage` and `containment`. Standard volume metrics will also be provided.\n\n* [2.1 Core Metrics](#core-metrics) - These are conversational metrics that apply to both chat and voice solutions.\n* [2.2 Voice Interaction Metrics](#voice-metrics) - Additional measurements for voice solutions including phone calls, call transfers, unique caller IDs, etc.\n* [2.3 Custom Metrics](#custom-metrics) - Other ad-hoc analysis. Requires knowledge of Python."}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.1 Core Metrics <a class=\"anchor\" id=\"core-metrics\"></a>\nThese metrics apply to all Watson Assistant solutions. For voice solutions, additional metrics are in the next section.\n* [2.1.1 Abandonment at Greeting](#abandonment)\n* [2.1.2 Coverage Metric](#coverage-metric)\n* [2.1.3 Search Skill Responses](#search-skill)\n* [2.1.4 Escalation Requests](#escalation-metric)\n* [2.1.5 Active Users](#active-users)\n* [2.1.6 Top Intents & Average Confidence Scores](#top-intents-scores)\n* [2.1.7 Top Entities](#top-entities)\n* [2.1.8 Optional: Bilingual Assistants](#bilingual-assistants)"}, {"metadata": {}, "cell_type": "code", "source": "# dict{} that we will send to CSV for use in Watson Studio Cognos Dashboard\nmetrics_dict = {}\n\n# These should match the count in the Watson Assistant Analytics tooling.\ntotalConvs = len(allLogsDF[conversationKey].unique())\nprint(\"Total messages:     \", len(allLogsDF))\nprint(\"Total conversations:\", totalConvs)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.1 Abandonment at Greeting <a class=\"anchor\" id=\"abandonment\"></a>\n\nThe logs include non-user messages such as welcome messages and system messages from a Voice Interaction solution. By filtering out these messages, it will reveal how many conversations abandoned before the first user utterance."}, {"metadata": {}, "cell_type": "code", "source": "# This removes blank inputs and vgwHangUp tags in log events\nfilteredLogsDF = allLogsDF[allLogsDF['input.text'] != \"\"]\nfilteredLogsDF = filteredLogsDF[filteredLogsDF['input.text'] != 'vgwHangUp'] \nfilteredLogsDF = filteredLogsDF[filteredLogsDF['input.text'] != 'vgwPostResponseTimeout'] \n\nfilteredMessages = len(filteredLogsDF)\nfilteredConvs = len(filteredLogsDF[conversationKey].unique())\nabandonedAtGreeting = (totalConvs - filteredConvs)\nmetrics_dict['abandonedAtGreeting'] = [abandonedAtGreeting] # Put into metrics dict\n\nprint(\"Abandoned conversations (no user input):\", abandonedAtGreeting)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.2 Coverage Metric <a class=\"anchor\" id=\"coverage-metric\"></a>\nCoverage is the measurement of the portion of total user messages that your assistant is attempting to respond to. For example, any messages that respond with \"Sorry I didn't understand\" from the anything_else node is considered uncovered.\n\n> **Action Required:** Define the node ids in `anything_else_nodes` list that represent any responses for uncovered messages. This can be found by exporting the Skill from the Assistant tooling, and searching the JSON for the relevant `dialog_node`. "}, {"metadata": {}, "cell_type": "code", "source": "# Define the node_id for anything_else and other uncovered nodes\nanything_else_nodes = ['XXXXXXXX'] \n\n# coveredDF = allLogsDF\nallLogsDF.rename(columns={'input.text': 'input_text'}, inplace=True)\ncoverage = []\n\nfor row in allLogsDF.itertuples():\n    appended = False \n    nodes = row.nodes_visited\n    for node in nodes:\n        if node in anything_else_nodes:\n            coverage.append('uncovered') # Mark as uncovered if message hit node in anything_else_nodes list\n            appended = True\n            break\n    if (row.input_text == '' or row.input_text == 'vgwHangUp' or row.input_text == 'vgwPostResponseTimeout') and not appended:\n        coverage.append('system_message') # Mark greetings and voicegateway actions as system_messages\n        appended = True\n    if not appended:\n        coverage.append('covered') # else, everything else is covered\n\nallLogsDF['coverage'] = coverage\nallLogsDF.rename(columns={'input_text': 'input.text'}, inplace=True)\ncoveredDF = allLogsDF[allLogsDF['coverage'] == 'covered']\nuncoveredDF = allLogsDF[allLogsDF['coverage'] == 'uncovered']\n\nprint('Covered messages:   ', len(coveredDF))\nprint('Uncovered messages: ', len(allLogsDF[allLogsDF['coverage'] == 'uncovered']))\nprint('System messages:    ', len(allLogsDF[allLogsDF['coverage'] == 'system_message']))\nprint('\\nCoverage metric:    ','{:.0%}'.format(len(coveredDF) / filteredMessages))\n\n# coveredMsgs[['input_text','output.text','coverage']].tail(10)\n\nmetrics_dict['coverage'] = [len(coveredDF) / filteredMessages] # Put into metrics dict", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# uncoveredDF[['input.text','output.text']].head(10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.2 Search Skill Responses <a class=\"anchor\" id=\"search-skill\"></a>\nWatson Assistant has multiple response types including `text`, `option`, `image`, `pause`, or `search skill`. Each of these types are marked within `output.generic.response_type` inside the log data. This cell will calculate the number of Search Skill responses."}, {"metadata": {}, "cell_type": "code", "source": "# Run this cell\nresponse_type = []\n\nfor row in allLogsDF['output.generic']:\n    search_skill = False\n    for response in row: # each output can have multiple responses\n        if response['response_type'] == 'search_skill':\n            response_type.append('search_skill')\n            search_skill = True\n            break\n                \n    if not search_skill: # if the response was not a search skill, append other to the list\n        response_type.append('other')\n        \nallLogsDF['response_type'] = response_type # Add in response_type column to allLogsDF\nsearchSkillDF = allLogsDF[allLogsDF['response_type'] == 'search_skill'] # Set new DF \nprint('Total Search Skill responses:',len(searchSkillDF))\nprint('Percentage of total messages: {:.0%}'.format(len(searchSkillDF) / len(allLogsDF) ))\n\nsearchSkillDF[['input.text','response_type']].head().reset_index(drop=True) # Print the list of user inputs that caused search skill", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Saves to CSV\nproject.save_data(file_name = custName + \"_search-skill-inputs.csv\",data = searchSkillDF.to_csv(index=False),overwrite=True); # This saves in COS. Comment out if running locally", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.3 Escalation Requests <a class=\"anchor\" id=\"escalation-metric\"></a>\n\nEscalation refers to any time a user is prompted to contact a live person (e.g. 1-800 number). If the assistant has an integration with a live handoff service (e.g. ZenDesk), this is considered escalation. For Voice Interaction solutions, we calculate `call containment` in the next section by counting the number of actual call transfers in the logs.\n\n> **Action Required:** Define the node id in `escalation_node` for a node that represents any responses to an escalation request (e.g. `#General-Agent-Escalation`). This can be found by exporting the Skill from the Assistant tooling, and searching the JSON for the relevant dialog_node.\n "}, {"metadata": {}, "cell_type": "code", "source": "# Define the escalation node\nescalation_node = \"XXXXXXXX\" \nnode_visits_escalated = allLogsDF[[escalation_node in x for x in allLogsDF['nodes_visited']]]\n\nescalationMetric = len(node_visits_escalated)/filteredMessages\nmetrics_dict['escalation'] = [escalationMetric] # Put into metrics dict\nprint(\"Total visits to escalation node:\",len(node_visits_escalated))\nprint(\"Percent of total messages escalated:\",'{:.0%}'.format(escalationMetric))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.4 Active Users <a class=\"anchor\" id=\"active-users\"></a>\nHow many unique users used the assistant?"}, {"metadata": {}, "cell_type": "code", "source": "uniqueUsers = allLogsDF[\"metadata.user_id\"].nunique()\nmetrics_dict['uniqueUsers'] = [uniqueUsers] # inserts into metrics dict\nprint('Total unique users: {}'.format(uniqueUsers))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.5 Top Intents & Average Confidence Scores <a class=\"anchor\" id=\"top-intents-scores\"></a>"}, {"metadata": {}, "cell_type": "code", "source": "# Using pandas aggregators to count how often each intent is selected and its average confidence\nintentsDF = filteredLogsDF.groupby('intent',as_index=False).agg({\n   'input.text': ['count'], \n   'intent_confidence': ['mean']\n})\n\nintentsDF.columns=[\"intent\",\"count\",\"confidence\"] #Flatten the column headers for ease of use\n\nintentsDF = intentsDF[intentsDF['intent'] !=''] # Remove blanks, usually VGW tags + greetings\nintentsDF = intentsDF.sort_values('count',ascending=False)\nintentsDF = intentsDF.reset_index(drop=True)\nintentsDF.head(5) # If you want specific number shown, edit inside head(). If you want to show all, remove head() ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#ax = sns.barplot(x=\"count\", y=\"intent\", data=intentsDF.head(),orient='h',palette=\"Blues_d\").set_title('Top Intents')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.6 Top Entities (Defunct) <a class=\"anchor\" id=\"top-entities\"></a>"}, {"metadata": {}, "cell_type": "code", "source": "entityDF = allLogsDF[allLogsDF[\"entities\"] != \"\"]\n#intentsDF = intentsDF[intentsDF['intent'] !=''] # Remove blanks, usually VGW tags + greetings\nentityDF[\"entities\"].iloc[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.7 Optional: Bilingual Assistants <a class=\"anchor\" id=\"bilingual-assistants\"></a>\nFor assistants that use a single skill for two different languages. The skill may set a context variable (e.g. `$language==\"english\"`) and then respond accordingly based on this variable. This cell will count the unique conversation_ids that have a given context variable.\n\n> **Optional:** Define the `languageVar` that your skill uses to identify the language used to respond to the user."}, {"metadata": {}, "cell_type": "code", "source": "languageVar = 'language' # define the context variable that you retrieved above in customFields\n\nlanguageDF = allLogsDF.groupby([languageVar])[\"conversation_id\"].nunique()\nlanguageDF = turn_dict_to_df(languageDF, ['Context Var', 'Count'])\nlanguageDF = languageDF[languageDF['Context Var'] != '']\nlanguageDF", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.2 Voice Interaction Metrics <a class=\"anchor\" id=\"voice-metrics\"></a>\nThese metrics are for Voice Agent solutions. We start with volume metrics. \nIf your solution is chat only, [skip to the next section.](#export-to-csv)\n\n* [2.2.1 Call Containment Rate](#containment-rate)\n* [2.2.2 Unique Callers](#unique-callers)\n* [2.2.3 SMS Sent](#sms-sent)"}, {"metadata": {}, "cell_type": "code", "source": "uniqueCallers = allLogsDF['vgwSIPFromURI'].unique()\nuniqueCalls = allLogsDF['vgwSessionID'].unique()\n\nprint(\"Total phone calls:\", len(uniqueCalls)) # It will print '1' if there are no calls found in the logs\nprint(\"Total unique callers:\", len(uniqueCallers))\nprint(\"Average messages per call:\", int(len(allLogsDF) / len(uniqueCalls)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Filters out blank inputs and vgwHangUp tags in log events\nfilteredLogsDF = allLogsDF[allLogsDF['input.text'] != \"\"]\nfilteredLogsDF = filteredLogsDF[filteredLogsDF['input.text'] != 'vgwHangUp'] \nfilteredLogsDF = filteredLogsDF[filteredLogsDF['input.text'] != 'vgwPostResponseTimeout'] ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2.1 Call Containment Rate <a class=\"anchor\" id=\"containment-rate\"></a>\nHow many call transfers did the voice solution perform?"}, {"metadata": {}, "cell_type": "code", "source": "transfersDF = allLogsDF.groupby([\"output.vgwAction.parameters.transferTarget\"])[\"vgwSessionID\"].count()\ntransfersDF = turn_dict_to_df(transfersDF, ['TransferTo', 'Count'])\ntransfersDF = transfersDF[transfersDF['TransferTo'] != '']\n\nprint('Call transfer count:', transfersDF['Count'].sum()) \ncontainmentRate = 1 - transfersDF['Count'].sum() / len(uniqueCalls)\nprint('Call containment rate:', '{:.0%}'.format(containmentRate))\nmetrics_dict['callTransfers'] = [transfersDF['Count'].sum()] # Put into metrics dict\nmetrics_dict['containment'] = [containmentRate] # Put into metrics dict\ntransfersDF.sort_values('Count',ascending=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2.2 Unique Callers <a class=\"anchor\" id=\"unique-callers\"></a>\nHow many unique caller IDs dialed into the voice solution?"}, {"metadata": {}, "cell_type": "code", "source": "callsDF = allLogsDF.groupby(['vgwSIPFromURI'])['vgwSessionID'].nunique()\ncallsDF = pd.DataFrame.from_dict(callsDF)\ncallsDF.reset_index(level=0, inplace=True)\ncallsDF.columns = ['Caller ID', 'Call Count']\nprint('Total unique caller IDs:', len(callsDF))\ncallsDF.head().sort_values('Call Count',ascending=False)\nmetrics_dict['callerIDs'] = [len(callsDF)] # Put into metrics dict", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2.3 SMS Sent <a class=\"anchor\" id=\"sms-sent\"></a>\nHow many SMS were sent by the assistant? A text message can be sent to the caller and can be initiated from within the Watson Assistant JSON editor. This will count the number of SMS sent."}, {"metadata": {}, "cell_type": "code", "source": "smsDF = allLogsDF[allLogsDF['vgwSMSUserPhoneNumber'] != '']\nmetrics_dict['sms'] = [len(smsDF)] # Put into metrics dict\nprint('Total SMS sent to callers: {}'.format(len(smsDF)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.3 Custom Metrics <a class=\"anchor\" id=\"custom-metrics\"></a>\nThis section is optional and can be used to create custom metrics. It will require the basic knowledge of Python and Pandas. Two examples of custom metrics included below can be modified, or additional metrics can be added here. [Jump to section 2.4](#export-logs) if you do not wish to build custom metrics.\n\n* [2.3.1 Context Variable Count](#context-variable-count)\n* [2.3.2 Response Mentions](#response-mentions)"}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3.1 Context Variable Count <a class=\"anchor\" id=\"context-variable-count\"></a>\nSome use cases require the use of context variables in order to track user inputs. For one customer, the assistant asks a series of questions in order to screen the patient. \n\n> **Optional:** If you wish to count the number of context variables used across unique conversation IDs, define `contextVar` below."}, {"metadata": {}, "cell_type": "code", "source": "contextVar = 'preferredContact' # define the context variable that you retrieved above in customFields\n\ncontextDF = allLogsDF.groupby([contextVar])[\"conversation_id\"].nunique()\ncontextDF = turn_dict_to_df(contextDF, ['Context Var', 'Count'])\ncontextDF = contextDF[contextDF['Context Var'] != '']\ncontextDF", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "contextVar = 'contactSubmitted' # define the context variable that you retrieved above in customFields\n\ncontextDF = allLogsDF.groupby([contextVar])[\"conversation_id\"].nunique()\ncontextDF = turn_dict_to_df(contextDF, ['Context Var', 'Count'])\ncontextDF = contextDF[contextDF['Context Var'] != '']\ncontextDF", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# project.save_data(file_name = custName + \"_ScreeningCount.csv\",data = customVarDF.to_csv(index=False),overwrite=True) # This saves in COS. Comment out if running locally", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3.2 Response Mentions <a class=\"anchor\" id=\"response-mentions\"></a>\nA specific customer wanted to identify all mentions of `311` in the responses to users. You can modify this or comment it out."}, {"metadata": {}, "cell_type": "code", "source": "helpDF = allLogsDF[(allLogsDF['output.text'].str.contains('311')) | (allLogsDF['output.text'].str.contains('3-1-1'))] \nprint('Total 3-1-1 response mentions:', len(helpDF))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 3. Export Logs  <a class=\"anchor\" id=\"export-logs\"></a>\nThe transformed log data inside the Pandas dataframe will be saved to CSV files and DB2 on Cloud database. These logs can be used for further data exploration and for creating visualizations in Cognos Dashboard in Watson Studio.\n\n* [3.1 Saving CSV files to Cloud Object Storage](#export-to-csv)  CSV files will be saved to the project's Data Assets and Cloud Object Storage.\n* [3.2 Loading into DB2 on Cloud database](#export-to-db2) The data will be saved to a table on your DB2 instance. \n\n## 3.1 Saving CSV files to Cloud Object Storage <a class=\"anchor\" id=\"export-to-csv\"></a>\nThe data will be saved into a CSV file in Cloud Object Storage, accessible via your project's assets folder in Watson Studio. There will be three distinct CSV files saved:\n* `_logs.csv` will contain all of the data within the allLogs dataframe\n* `_KeyMetrics.csv` will contain the calculated metrics such as coverage, escalation, containment rate, etc.\n* `_uncovered_msgs.csv` will contain the selection of uncovered messages. This file can be used for making improvements to intent training and dialog responses.\n\n\n### 3.1.1 Save all logs to CSV"}, {"metadata": {}, "cell_type": "code", "source": "# allLogsDF.to_csv(custName+'_logs.csv',index=False) # This saves if running notebook locally. Comment out for Studio. \nprint('Saving all logs to {}'.format(custName+ \"_logs.csv\"))\nproject.save_data(file_name = custName + \"_logs.csv\",data = allLogsDF.to_csv(index=False),overwrite=True); # This saves in COS. Comment out if running locally", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1.2 Save KPIs to CSV"}, {"metadata": {}, "cell_type": "code", "source": "metricsDF = pd.DataFrame(metrics_dict)\n# metricsDF.to_csv(custName + \"_KeyMetrics.csv\",index=False) # This saves if running notebook locally. Comment out for Studio. \nproject.save_data(file_name = custName + \"_KeyMetrics.csv\",data = metricsDF.to_csv(index=False),overwrite=True); # This saves in COS. Comment out if running locally\nprint('Saving key metrics to {}'.format(custName+ \"_KeyMetrics.csv\"))\nmetricsDF", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1.3 Save uncovered messages to CSV\nImprove Coverage by analyzing these uncovered messages. This might require adding training data to Intents or customizing STT models."}, {"metadata": {}, "cell_type": "code", "source": "print('\\nSaved', len(uncoveredDF), 'messages to:', custName + \"_uncovered_msgs.csv\")\n# uncoveredDF.to_csv(custName + \"_uncovered_msgs.csv\",index=False, header=['Utterance','Response','Intent','Confidence'])\n\nproject.save_data(file_name = custName + \"_uncovered_msgs.csv\",data = uncoveredDF.to_csv(index=False),overwrite=True); # This saves in COS. Comment out if running locally", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3.2 Loading into DB2 on Cloud database <a class=\"anchor\" id=\"export-to-db2\"></a>\nThe transformed log data will be inserted into a table within an instance of DB2 on Cloud. Requires configuration in section 1.1."}, {"metadata": {}, "cell_type": "code", "source": "# Prepare to create Logs\ncolumns = 'BRANCH_EXITED_REASON,CONVERSATION_ID,DIALOG_TURN_COUNTER,ENTITIES,INPUT_TEXT,INTENT,INTENT_CONFIDENCE,NODES_VISITED,OUTPUT_TEXT,REQUEST_TIMESTAMP,RESPONSE_TIMESTAMP,LANGUAGE,PREV_NODES_VISITED,MONTH,DAY,YEAR,CALLER_ID,VGW_SESSION_ID,SMS_NUMBER,CALL_TRANSFER,USER_ID,COVERAGE,RESPONSE_TYPE,INPUT_BIGRAMS'\ninsertSQL = 'Insert into WATSON.WA_FULL_LOGS(' + columns + ') values(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)'\nstmt = ibm_db.prepare(conn, insertSQL)\ncheckSQL = 'Select CONVERSATION_ID,RESPONSE_TIMESTAMP from WATSON.WA_FULL_LOGS where CONVERSATION_ID = ? and RESPONSE_TIMESTAMP = ?'\ncheckStmt = ibm_db.prepare(conn, checkSQL)\ninsertSQL", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "LOG_EVENTS_COUNTER = 0\n# Insert the rows from the dataframe into the DB2 table.\nfor n in range(len(allLogsDF)) :\n    # check whether this record exists conversation id, response time stamp\n    ibm_db.bind_param(checkStmt,1,allLogsDF.at[n,'conversation_id'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n    ibm_db.bind_param(checkStmt, 2,allLogsDF.at[n,'response_timestamp'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_TYPE_TIMESTAMP)\n    #select_stmt = ibm_db.exec_immediate(conn, select_sql)\n    select_result = ibm_db.execute(checkStmt)\n    row_exists = ibm_db.fetch_both(checkStmt)\n    if row_exists == False :\n        #Row does not exists, hence insert into the All logs table\n        LOG_EVENTS_COUNTER = LOG_EVENTS_COUNTER + 1\n        #print('Inserting conversation id: ' + allLogsDF.at[n,'conversation_id'])\n        \n        ibm_db.bind_param(stmt,1,allLogsDF.at[n,'branch_exited_reason'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt,2,allLogsDF.at[n,'conversation_id'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt,3,allLogsDF.at[n,'dialog_turn_counter'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_DECIMAL)\n        ibm_db.bind_param(stmt,4,str(allLogsDF.at[n,'entities'][:1020]), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt,5,str(allLogsDF.at[n,'input.text'][:1020]), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt,6,str(allLogsDF.at[n,'intent'][:1020]), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt,7,allLogsDF.at[n,'intent_confidence'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_DECIMAL)\n        ibm_db.bind_param(stmt, 8,str(allLogsDF.at[n,'nodes_visited'][:1020]), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt, 9,str(allLogsDF.at[n,'output.text'][:1020]), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt, 10,allLogsDF.at[n,'request_timestamp'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_TYPE_TIMESTAMP)\n        ibm_db.bind_param(stmt, 11,allLogsDF.at[n,'response_timestamp'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_TYPE_TIMESTAMP)\n\n        ibm_db.bind_param(stmt, 12,str(allLogsDF.at[n,'language'][:1020]), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt, 13,str(allLogsDF.at[n,'prev_nodes_visited'][:1020]), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n         \n        ibm_db.bind_param(stmt, 14,str(allLogsDF.at[n,'month']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_INTEGER)\n        ibm_db.bind_param(stmt, 15,str(allLogsDF.at[n,'day']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_INTEGER)\n        ibm_db.bind_param(stmt, 16,str(allLogsDF.at[n,'year']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_INTEGER)\n\n        ibm_db.bind_param(stmt, 17,str(allLogsDF.at[n,'vgwSIPFromURI']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt, 18,str(allLogsDF.at[n,'vgwSessionID']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt, 19,str(allLogsDF.at[n,'vgwSMSUserPhoneNumber']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt, 20,str(allLogsDF.at[n,'output.vgwAction.parameters.transferTarget']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\n        ibm_db.bind_param(stmt, 21,str(allLogsDF.at[n,'metadata.user_id']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)       \n        ibm_db.bind_param(stmt, 22,str(allLogsDF.at[n,'coverage']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR) \n        ibm_db.bind_param(stmt, 23,str(allLogsDF.at[n,'response_type']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR) \n        ibm_db.bind_param(stmt, 24,str(allLogsDF.at[n,'input_bigrams']), ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR) \n        \n        ibm_db.execute(stmt)\n#ibm_db.commit(conn)\n#ibm_db.close(conn)\nprint('Total log events saved to database:', LOG_EVENTS_COUNTER)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Update the current run details (WA_LAST_RUN_LOG)"}, {"metadata": {}, "cell_type": "code", "source": "# Store Current run details\ndel_tracking = 'Delete from WATSON.WA_LAST_RUN_LOG'\ninsert_tracking = 'Insert into WATSON.WA_LAST_RUN_LOG (conversation_id, request_timestamp, response_timestamp, lastrun_timestamp) Values (?,?,?,?) '\ntrans_stmt = ibm_db.prepare(conn, insert_tracking)\ninsert_tracking", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Delete previous entry\nibm_db.exec_immediate(conn,del_tracking)\n\n# Get the latest log entry from the dataframe. First let's sort it so tail(1) is the last entry.\nallLogsDF.sort_values(by=['response_timestamp'], axis=0, \n                 ascending=True, inplace=True)\nallLogsDF = allLogsDF.reset_index(drop=True)\nlast_row = allLogsDF.tail(1).reset_index(drop=True)\n#store the latest row details.\nibm_db.bind_param(trans_stmt,1,last_row.at[0,'conversation_id'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_VARCHAR)\nibm_db.bind_param(trans_stmt,2,last_row.at[0,'request_timestamp'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_TYPE_TIMESTAMP)\nibm_db.bind_param(trans_stmt,3,last_row.at[0,'response_timestamp'], ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_TYPE_TIMESTAMP)\nibm_db.bind_param(trans_stmt,4,pd.Timestamp.now(),ibm_db.SQL_PARAM_INPUT,ibm_db.SQL_TYPE_TIMESTAMP)\nibm_db.execute(trans_stmt)\nprint(pd.Timestamp.now())\n#Commit and close the connection\nibm_db.commit(conn)\nibm_db.close(conn)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### End of Notebook v2.1 (last modified on 7-2-20)"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}